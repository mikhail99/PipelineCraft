"""Mock LLM service for report generation."""

from typing import List
from .models import Paper


def generate_report_content(
    papers: List[Paper],
    template_prompt: str,
    user_prompt: str | None = None
) -> str:
    """
    Mock LLM report generation.
    
    In a real implementation, this would call an LLM API (OpenAI, Anthropic, etc.)
    with the papers content and prompts.
    """
    # Build a summary of papers for the mock
    paper_summaries = []
    for p in papers:
        authors = p.authors or "Unknown authors"
        date = p.publish_date.strftime("%Y-%m-%d") if p.publish_date else "Unknown date"
        paper_summaries.append(f"- **{p.title}** by {authors} ({date})")
    
    papers_list = "\n".join(paper_summaries) if paper_summaries else "_No papers in selected libraries_"
    
    # Mock generated report
    report_content = f"""# Generated Report

## Overview

This report was generated based on **{len(papers)}** papers from the selected libraries.

### Template Used
> {template_prompt}

{f"### User Request" + chr(10) + f"> {user_prompt}" + chr(10) if user_prompt else ""}

---

## Papers Analyzed

{papers_list}

---

## Summary

This is a **mock report** generated by the PipelineCraft system. In a production environment, 
this content would be generated by an LLM (such as GPT-4, Claude, or similar) that would:

1. Analyze the full text of all papers
2. Follow the template instructions
3. Generate insights based on the user's prompt
4. Produce a comprehensive literature review or analysis

---

## Key Findings

> ⚠️ **Note**: This is placeholder content. Connect an LLM API to generate real insights.

Based on analysis of the {len(papers)} papers:

- **Common Themes**: To be generated by LLM
- **Research Gaps**: To be generated by LLM  
- **Recommendations**: To be generated by LLM

---

*Report generated at: {{timestamp}}*
"""
    
    from datetime import datetime
    return report_content.replace("{{timestamp}}", datetime.utcnow().isoformat())
